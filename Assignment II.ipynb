{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWhQdWO_4qqe"
   },
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <h1></h1>\n",
    "  <h1>Stylized Retrieval-Augmented Generation</h1>\n",
    "  <h4 align=\"center\">Assignmnet II</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi69xdNwKZPv"
   },
   "source": [
    "Welcome to Assignment II! In this notebook, you will build and implement a Retrieval-Augmented Generation (RAG) pipeline tailored for a text style transfer application.\n",
    "\n",
    "**By the end of this assignment, you'll be able to:**\n",
    "\n",
    "*   Build a Retrieval-Augmented Generation (RAG) pipeline to enhance text generation with external knowledge.\n",
    "*   Retrieve relevant information from a dataset or knowledge base to support text generation.\n",
    "*   Implement a neural style transfer model to transform text into a desired writing style.\n",
    "*   Combine retrieved content and style transfer to create a coherent and stylistically customized output.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUWduwxuKgBa"
   },
   "source": [
    "## Important Note on Submission\n",
    "\n",
    "\n",
    "*   Do not use ChatGPT or any other AI tool to directly produce the code. If you need assistance, refer to Exercise 5 for guidance.\n",
    "*   You are allowed to work in a group of up to 3 members.\n",
    "*   Do not copy code or answers from other groups. Collaboration is encouraged only within your own group.\n",
    "*   Ensure that your notebook is runnable without any errors. Submissions with errors will not be accepted.\n",
    "*   Answers to open-ended questions must be original and not copied from other groups or AI tools like ChatGPT.\n",
    "*   The submission should be one .ipynb notebook with the group members' names on Openlat and matriculation numbers on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_gXYEhB7WzW"
   },
   "source": [
    "## Group Members\n",
    "\n",
    "\n",
    "1. First memebr: \n",
    "  * Name: Hoang Long Nguyen\n",
    "  * Matrikel-Nr.: 428832\n",
    "2. Second memebr:\n",
    "  * Name: Mateen Mahmood\n",
    "  * Matrikel-Nr.: 426365\n",
    "2. Third memebr:\n",
    "  * Name:Vibha Kedigemane Trivikram\n",
    "  * Matrikel-Nr.: 429106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bss2qVkJKkZr"
   },
   "source": [
    "### Table of Contents\n",
    "- [1. Access to Hugging Face](#1-access-to-hugging-face)\n",
    "- [2. Packages](#2-packages)\n",
    "- [3. Problem Statement](#3-problem-statement)\n",
    "- [4. Fetch and Parse](#4-fetch-and-parse)\n",
    "- [5. Calculate Word Stats](#5-calculate-word-stats)\n",
    "- [6. Set Up LLM](#6-set-up-llm)\n",
    "- [7. BM25 Retriever](#7-bm25-retriever)\n",
    "- [8. Build Chroma](#8-build-chroma)\n",
    "- [9. Ensemble Retriever](#9-ensemble-retriever)\n",
    "- [10. Format Documents](#10-format-documents)\n",
    "- [11. RAG Chain](#11-rag-chain)\n",
    "- [12. Final Response](#12-final-response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZWrcJ4LKneE"
   },
   "source": [
    "# 1. Access to Hugging face\n",
    "Execute the following cell to connect to your Hugging Face account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pe3-F-kiKYiD"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Prompt user for Hugging Face API token if not already set\n",
    "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Huggingfacehub API token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N72UZSMKqSa"
   },
   "source": [
    "# 2. Packages\n",
    "Execute the following code cells for installing the packages needed for creating your Stylized RAG.\n",
    "\n",
    "note: If there are package conflics you can use pip-tools to automatically find and install the compatible versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WLei89zCKsEj"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-community\n",
    "# !pip install langchain-huggingface\n",
    "# !pip install bs4\n",
    "# !pip install rank_bm25\n",
    "# !pip install huggingface_hub\n",
    "# !pip install requests\n",
    "# !pip install langchain-chroma\n",
    "# pip install transformers==4.46.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2K3p5lXKuSy"
   },
   "source": [
    "# 3. Problem Statement\n",
    "In this assignment, we will implement **Text Style Transfer**, a technique that modifies text style while preserving its content. They will build an **ensemble retriever** combining **BM25** for keyword-based retrieval and **Chroma** for semantic search to retrieve relevant documents, which will be used as input for the style transfer process. This project integrates classical retrieval methods with modern neural embeddings for practical NLP applications.\n",
    "\n",
    "**what is text style transfer?**\n",
    "\n",
    "**Text Style Transfer** is a natural language processing (NLP) technique that modifies the style of a given text while preserving its original content. It allows for the transformation of linguistic expressions to convey different tones, emotions, or writing styles without altering the underlying meaning. For example, it can rephrase formal text into a casual tone, adapt neutral statements into an emotional tone, or convert modern language into a Shakespearean style. This technique has applications in personalized communication, creative writing, sentiment adjustment, and even domain adaptation, making it a powerful tool for generating diverse textual outputs tailored to specific needs.\n",
    "\n",
    "### Example of Text Style Transfer:\n",
    "\n",
    "#### **Input (Neutral Tone):**\n",
    "\"I am excited about the opportunity to work on this project.\"\n",
    "\n",
    "#### **Output (Formal Tone):**\n",
    "\"I am genuinely enthusiastic about the prospect of contributing to this project.\"\n",
    "\n",
    "#### **Output (Casual Tone):**\n",
    "\"I'm super pumped to get started on this project!\"\n",
    "\n",
    "#### **Output (Shakespearean Style):**\n",
    "\"Verily, I am thrilled by the chance to partake in this noble endeavor.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQc6rEV4MHfY"
   },
   "source": [
    "# 4. Fetch and Parse\n",
    "In this part of the assignment, you are tasked with:\n",
    "\n",
    "*    Fetching and parsing web content: Write a function that fetches the HTML content of a webpage and processes it to extract clean, readable text.\n",
    "*    Splitting text into smaller chunks: Implement a function to split the text into overlapping chunks, ensuring that each chunk is manageable for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0kxayMljLPhu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def fetch_and_parse(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the webpage content at `url` and return a cleaned string of text.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the webpage to fetch.\n",
    "\n",
    "    Returns:\n",
    "    - str: Cleaned text content extracted from the webpage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Fetch the webpage content using the requests library.\n",
    "    # Fetch the content of the URL.\n",
    "    # Ensure the request is successful.\n",
    "    \n",
    "    # Step 2: Parse the HTML content using BeautifulSoup.\n",
    "\n",
    "    # Step 3: Extract the text content from the parsed HTML.\n",
    "\n",
    "    # Step 4: Return the cleaned text.\n",
    "\n",
    "    # Write your code here.\n",
    "    try:\n",
    "        # Step 1\n",
    "        fetch = requests.get(url, timeout = 10)\n",
    "        if fetch.status_code != 200:\n",
    "            print(f\"Failed to fetch URL: {url} with status code: {fetch.status_code}\")\n",
    "            return \"\"\n",
    "        # Step 2\n",
    "        soup = BeautifulSoup(fetch.content, 'html.parser')\n",
    "        # Step 3\n",
    "        cleaned_text = soup.get_text()\n",
    "        # Step 4\n",
    "        return cleaned_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_text_into_documents(text: str, chunk_size: int = 500, chunk_overlap: int = 100):\n",
    "    \"\"\"\n",
    "    Split a long text into overlapping chunks and return them as a list of Documents.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The long text to split.\n",
    "    - chunk_size (int): The size of each chunk (default is 1000 characters).\n",
    "    - overlap (int): The number of overlapping characters between consecutive chunks (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of Documents, each containing a chunk of text.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    # Initialize an empty list to store the chunks.\n",
    "    step = chunk_size - chunk_overlap\n",
    "    for i in range(0, len(text), step):\n",
    "        chunk = text[i:i+chunk_size]\n",
    "        docs.append(Document(page_content=chunk))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Scraping content and splitting into documents...\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Machine_learning\n",
      "<class 'list'>\n",
      "Total number of documents: 820\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Scraping content and splitting into documents...\")\n",
    "example_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "]\n",
    "\n",
    "# Step 1A: Initialize an empty list to store all documents\n",
    "all_docs = []\n",
    "\n",
    "# Step 1B: Iterate through the URLs to fetch and process content\n",
    "for url in example_urls:\n",
    "    print(f\"Scraping content from: {url}\")\n",
    "\n",
    "    # Step 1B.1: Fetch and parse the raw text from the URL\n",
    "    raw_text = fetch_and_parse(url)\n",
    "\n",
    "    # Step 1B.2: Split the raw text into chunks (documents)\n",
    "    splits = split_text_into_documents(raw_text)\n",
    "\n",
    "    # Step 1B.3: Add the chunks to the list of documents\n",
    "    all_docs.extend(splits)\n",
    "print(type(all_docs))\n",
    "\n",
    "print(f\"Total number of documents: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC4W_opcOi1o"
   },
   "source": [
    "1. Why do we split the text into smaller chunks before storing or processing it?\n",
    "\n",
    "Answer: In order to avoid the risk of exceeding the LLMâ€™s context window, we have to split the text into smaller chunks before storing and processing. Splitting the text into chunks ensures that the documents are workable for retrieval and LLM input. Splitting gives context to these chunks which makes easy to retrieve the text we require. So we do it for contextual clarity, indexing and avoiding the input size limits of model.\n",
    "\n",
    "2. What challenges might you face when fetching and parsing web content, and how would you handle them?\n",
    "\n",
    "Answer: Websites do have different structure which makes it difficult to extract the information, for this we can use python libraries i.e. BeautifulSoup which scrapes the web content and handles varied structure.  Websites do have rate-limiting requests set to block or limit the scraping attemps. we can use headers to mimic the behaviour of browser and use request throttling. Another challenge we might face is broken or invalid URLS which lead to error codes. In order to handle this, we have implemented the error handling for fetching. \n",
    "\n",
    "3. In the context of RAG, how would errors in the fetch_and_parse function affect the overall pipeline?\n",
    "\n",
    "Answer: If we have errors in fetch and parse function in our RAG pipeline then the retrieved answers may be incomplete and inaccurate. it affects the pipeline ability to retrieve relevant information which we are looking for. it may produce low-quality embeddings which will effect the retrieval process and resulting into generating irrelevant content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmZQ8HhGPugo"
   },
   "source": [
    "# 5. Calculate Word Stats\n",
    "\n",
    "In this task, you will implement a function to calculate basic word and character statistics for a list of documents. Each document is represented as a Document object with a page_content attribute that contains its text.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Calculate the total number of words and characters across all documents.\n",
    "2. Compute the average number of words and characters per document.\n",
    "3. Print the average statistics in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BkmqrEeYM-8z"
   },
   "outputs": [],
   "source": [
    "def calculate_word_stats(texts):\n",
    "    \"\"\"\n",
    "    Calculate and display average word and character statistics for a list of documents.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list): A list of Document objects, where each Document contains a `page_content` attribute.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints the average word and character counts per document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize variables to keep track of total words and total characters.\n",
    "    total_words, total_characters = 0, 0\n",
    "\n",
    "    # Step 2: Iterate through each document in the `texts` list.\n",
    "    for doc in texts:\n",
    "        content = doc.page_content\n",
    "        word_count = len(content.split())\n",
    "        char_count = len(content)\n",
    "        total_words += word_count\n",
    "        total_characters += char_count\n",
    "\n",
    "    # Step 3: Calculate the average words and characters per document.\n",
    "    # - Avoid division by zero by checking if the `texts` list is not empty.\n",
    "    num_docs = len(texts)\n",
    "    avg_words = total_words / num_docs if num_docs > 0 else 0\n",
    "    avg_characters = total_characters / num_docs if num_docs > 0 else 0\n",
    "\n",
    "    # Step 4: Print the calculated averages in a readable format.\n",
    "    # Example: \"Average words per document: 123.45\"\n",
    "    print(f\"Average words per document: {avg_words}\")\n",
    "    print(f\"Average characters per document: {avg_characters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EpNcNKHdh_ar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words per document: 7.75\n",
      "Average characters per document: 44.5\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to test your calculate_word_stats function.\n",
    "# Create sample Document objects with text content for testing your code above.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"This is the first test document.\"),\n",
    "    Document(page_content=\"Here is another example document for testing.\"),\n",
    "    Document(page_content=\"Short text.\"),\n",
    "    Document(page_content=\"This document has more content. It's longer and has more words in it for testing purposes.\"),\n",
    "]\n",
    "\n",
    "# Call the function with the sample documents to calculate word statistics.\n",
    "calculate_word_stats(sample_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zzYkovNisNP"
   },
   "source": [
    "1. What potential issues could arise if the texts list is empty or contains documents with no content, and how would you address them?\n",
    "\n",
    "Answer: If the text list is empty then total_words / num_docs or total_characters / num_docs will give us the division by zero error. If there is a empty document then our calculated avaerage words and characters will be misleading mainly in datasets with many empty documents. In order to address them, we could check whether our list is empty or not before performing the calculation if num_docs > 0 else 0.  Also, we can skip the documents with empty content during the iteration 'if not content.strip(): continue'\n",
    "\n",
    "2. Why is it beneficial to calculate both word count and character count instead of just one of them?\n",
    "\n",
    "Answer: It is beneficial because language models work on tokenizd inputs, and the character count works as a proxy for calculating the token usage. If we only count the word then it might not capture the \n",
    "tokenization cost especially when we have special characters. If the document have high word count with low character count then it might it indicate the shorter and simpler words and the higher character count with low word count indicate complex words. The character count gives more detail about the size of the text including spaces and punctuation and helps in finding the anomalies in the text. \n",
    "Finally, words count are useful for readability and summarization tasks and character counts are useful for token usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIIdfz7tlPdG"
   },
   "source": [
    "# 6. Set Up LLM\n",
    "\n",
    "In this part of the assignment, you will implement a function to set up a Large Language Model (LLM) using the Hugging Face Endpoint API. This function will:\n",
    "\n",
    "1. Initialize and connect to a pre-trained model available on Hugging Face.\n",
    "2. Allow customization of parameters like the model repository ID and generation temperature.\n",
    "3. Return the configured LLM object, which will be used later for text generation tasks in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GnwtA4xolKWt"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "def setup_llm(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Set up and return a Hugging Face LLM using the specified model repository ID and generation parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - repo_id (str): The repository ID of the Hugging Face model to use (default: \"mistralai/Mistral-7B-Instruct-v0.3\").\n",
    "    - temperature (float): The generation temperature to control creativity in outputs (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "    - HuggingFaceEndpoint: A configured LLM object ready for text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Import the HuggingFaceEndpoint class.\n",
    "    # - This class allows you to connect to a Hugging Face model hosted on an endpoint.\n",
    "\n",
    "    # Step 2: Configure the LLM connection.\n",
    "    # - Use the HuggingFaceEndpoint class to set up the LLM.\n",
    "\n",
    "    # Step 3: Return the configured LLM object.\n",
    "    # - The returned LLM can be used for generating text based on input prompts.\n",
    "\n",
    "    # Write your code here.\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=repo_id,\n",
    "        temperature = 1.0,\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQu0sETym99k"
   },
   "source": [
    "1. What would happen if the temperature is set to an extreme value (e.g., 0 or 10)? How would you prevent misuse?\n",
    "\n",
    "Answer: If the temperature set to extreme value (e.g., 0 or 10) then it will b either too deterministic (e.g. 0) or it will generate highly random outputs (e.g. 10). In case of 0, the model will be\n",
    "generating predictable and repetitive responses and creativity of the model will be lower. However, if the temperature =0, the probability distribution is heavily skewed towards less probable tokens leading to inconsistant responses. In order to prevent this, we can set the default temperature range for example (0.5-1.2), and validation to check temperature is within the acceptable limits. \n",
    " \n",
    "2. If the LLM generates incorrect or irrelevant responses, what steps would you take to diagnose and fix the issue?\n",
    "\n",
    "Answer: To diagnose the issue, we can implement tests across all tasks, print out the results and analyze where the problem reside. In this assignment, one thing that we notice that make a great impact on the outout is the split_text_to_documents method. While, using the RecursiveCharacterTextSplitter(), the results is great. But as soon as we change to the for loops to perform chunking, we run into issues where the results produces nonsensical texts, we mitigate this problem by focusing on choosing the right values for chunk_size and chunk_overlap. Finally, we arrive at the values for them, 500 and 100, respectively. The results still produce nonsense sentences, but it's largely reduced in comparison to other chunking values. We understand that the prebuilt method like RecursiveCharacterTextSplitter produces more meaningful chunk of documents with complete sentences. With a simple for loops, the stucture is rigid and we can't take into account the structure of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdEyYy86oxn9"
   },
   "source": [
    "# 7. BM25 Retriever\n",
    "\n",
    "In this task, students will implement a BM25 Retriever, a critical component of the RAG pipeline.\n",
    "Your task is to:\n",
    "\n",
    "1. Initialize the BM25 retriever with a set of documents.\n",
    "2. Implement a method to retrieve the top k most relevant documents for a given query.\n",
    "3. Use efficient tokenization and scoring to ensure accurate and fast results.\n",
    "This component will enable the pipeline to fetch relevant information from a corpus, which is then passed to the LLM for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kp7Th24kmKLm"
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    A class to implement BM25-based document retrieval.\n",
    "\n",
    "    Attributes:\n",
    "    - documents (list): A list of Document objects.\n",
    "    - corpus (list): A list of strings representing the document contents.\n",
    "    - tokenized_corpus (list): A list of tokenized documents (lists of words).\n",
    "    - bm25 (BM25Okapi): The BM25 retriever initialized with the tokenized corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents):\n",
    "        \"\"\"\n",
    "        Initialize the BM25 retriever with the given documents.\n",
    "\n",
    "        Parameters:\n",
    "        - documents (list): A list of Document objects.\n",
    "        \"\"\"\n",
    "        # Step 1: Store the input documents.\n",
    "        # Hint: Use the `page_content` attribute of each Document object to extract text.\n",
    "        # Step 2: Tokenize the corpus.\n",
    "        # Hint: Use the `.split()` method to tokenize each document into words.\n",
    "        # Step 3: Initialize the BM25 retriever with the tokenized corpus.\n",
    "        self.documents = documents\n",
    "        self.corpus = [doc.page_content for doc in documents]\n",
    "        self.tokenized_corpus = [doc.lower().split() for doc in self.corpus]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve the top `k` most relevant documents for a given query.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input query as a string.\n",
    "        - k (int): The number of top documents to return (default is 5).\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of the top `k` relevant documents as strings.\n",
    "        \"\"\"\n",
    "        # Step 1: Tokenize the input query.\n",
    "        # Hint: Use `.split()` to tokenize the query into words.\n",
    "        # Step 2: Use the BM25 retriever to score and rank documents.\n",
    "        # Hint: Use the `bm25.get_top_n()` method to retrieve the top `k` documents.\n",
    "        # Step 3: Return the top `k` relevant documents.\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_k_results = self.bm25.get_top_n(tokenized_query, self.corpus, n=k)\n",
    "        return bm25_k_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skPWlylvpWgB"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qrEkVO9UpUtk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Relevant Documents:\n",
      "1. Machine learning is a method of data analysis that automates analytical model building.\n",
      "2. Deep learning is a subset of machine learning that uses neural networks with three or more layers.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample Document objects.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning is a method of data analysis that automates analytical model building.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning that uses neural networks with three or more layers.\"),\n",
    "    Document(page_content=\"Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing is a field of AI focused on the interaction between computers and human language.\"),\n",
    "]\n",
    "\n",
    "# Initialize the retriever with the sample documents.\n",
    "retriever = BM25Retriever(sample_docs)\n",
    "\n",
    "# Test the retriever with a query.\n",
    "query = \"What is machine learning?\"\n",
    "top_docs = retriever.retrieve(query,k=2)\n",
    "\n",
    "# Print the results.\n",
    "print(\"Top Relevant Documents:\")\n",
    "for idx, doc in enumerate(top_docs, 1):\n",
    "    print(f\"{idx}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmdT52BtqwfQ"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "Top Relevant Documents:\n",
    "1. Machine learning is a method of data analysis that automates analytical model building.\n",
    "2. Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s42FiJ8rEOE"
   },
   "source": [
    "1. If two documents have identical content except for minor differences (e.g., synonyms or paraphrasing), how might BM25 handle this, and why?\n",
    "\n",
    "Answer: If two documents have identical content except for minor differences like paraphrasing (e.g., \"AI\" vs. \"artificial intelligence\") then the document with exact matches will score higher. BM25 matches words in the query to the words in document. So, if the query is \"what is artificial intelligence then the BM25 will match these document which has the words artificial intelligence rather than matching wih the paraphrasing term \"AI\". The BM25 will rank the paraphrased documents lower as compared to exact matches becasue BM25 ranks documents on term frequency. Hence, we can say a small differences in words choice can lead to small deviations in BM25 scores.\n",
    "\n",
    "2. What challenges might arise if the corpus contains very short or very long documents? How would you address these challenges?\n",
    "\n",
    "Answer: with short documents, you'd face the issue where the context of the documents is not clear, leading to low scoring on all documents, for example, if the user ask a complex questions, the term frequency can be low, so the corpus needs to contain enough information that allow the query to be compared to the corpus's context. To handle this issue, we can increase the length of the corpus, or we group the smaller documents into larger groups with context similarity.\n",
    "\n",
    "With very long documents, the data can be \"watered down\", meaning the significance of important words is reduced, for example, if a corpus contain too many words like 'is', 'are', 'a', 'the', etc. these words can make the key words lose their value. We handle this problem by breaking down large documents into smaller chunk, we can use prebuilt methods that can help us identify a good breaking point to preverse meangingful context within the chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZhBH_Aqs2of"
   },
   "source": [
    "# 8. Build Chroma\n",
    "In this task, students will implement a function to build a Chroma vector store, a key component of the RAG pipeline. The Chroma vector store enables efficient semantic search by embedding documents into a high-dimensional vector space. Using these embeddings, the retriever can find documents that are semantically similar to a given query.\n",
    "\n",
    "The task involves:\n",
    "\n",
    "1. Initializing a vector store (Chroma) with Hugging Face embeddings.\n",
    "2. Adding a list of documents to the vector store.\n",
    "3. Returning the vector store for later use in the retrieval and generation pipeline.\n",
    "\n",
    "This function sets up the semantic retrieval system, allowing for more meaningful and context-aware results than keyword-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YiAtoavjpgPt"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "def build_chroma(documents: list[Document]) -> Chroma:\n",
    "    \"\"\"\n",
    "    Build a Chroma vector store using Hugging Face embeddings\n",
    "    and add the documents to it.\n",
    "\n",
    "    Parameters:\n",
    "    - documents (list[Document]): A list of Document objects to add to the vector store.\n",
    "\n",
    "    Returns:\n",
    "    - Chroma: The Chroma vector store containing the embedded documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize Hugging Face embeddings.\n",
    "    # - Use a pre-trained embedding model (e.g., \"sentence-transformers/all-mpnet-base-v2\").\n",
    "    # - HuggingFaceEmbeddings generates dense vector representations for text.\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    # Step 2: Initialize the Chroma vector store.\n",
    "    # - Set the collection name for the vector store (e.g., \"EngGenAI\").\n",
    "    # - Pass the Hugging Face embeddings as the embedding function.\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"EngGenAI\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "\n",
    "    # Step 3: Add the input documents to the Chroma vector store.\n",
    "    # - Use the `add_documents` method to embed and store the documents.\n",
    "    vector_store.add_documents(documents=documents)\n",
    "    # Step 4: Return the Chroma vector store for later use.\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivy_VQFItWTr"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-0Vb-uDutN3-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/8xy4bq9x7lld84_pt1wqbm4r0000gn/T/ipykernel_18090/3267756400.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "/opt/anaconda3/envs/python311_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/l0/8xy4bq9x7lld84_pt1wqbm4r0000gn/T/ipykernel_18090/3267756400.py:25: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store built successfully!\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x11c973210>\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample Document objects.\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning is a method of data analysis that automates analytical model building.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning that uses neural networks with three or more layers.\"),\n",
    "    Document(page_content=\"Artificial intelligence encompasses a wide range of technologies, including machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing is a field of AI focused on the interaction between computers and human language.\"),\n",
    "]\n",
    "\n",
    "# Call the function to build the Chroma vector store.\n",
    "vector_store = build_chroma(sample_docs)\n",
    "\n",
    "# Test retrieval (optional, if supported).\n",
    "print(\"Vector store built successfully!\")\n",
    "print(vector_store)  # Print the vector store object to verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TgIqUZGttLE"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "Vector store built successfully!\n",
    "\n",
    "<langchain.vectorstores.Chroma object at 0x7f8c1a4b3f10>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfDwKvbcvHvN"
   },
   "source": [
    "1. What happens if two documents have identical embeddings? How would you handle this in the retrieval process?\n",
    "\n",
    "Answer:  Two documents having identical embeddings means that they have the same vector in the database. so when we perform retrieval process, both of them are matches for the same query. in the retrieval process, even though they have the same embeddings, they have different ID and metadata. Or if we can compare the semantic content of the two documents, if they are likely to be the same, we can simply get rid of one and keep the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgxRBNaxv-ro"
   },
   "source": [
    "# 9. Ensemble Retriever\n",
    "\n",
    "In this task, students will implement an Ensemble Retriever that combines the strengths of Chroma (semantic similarity) and BM25 (keyword-based retrieval) to create a hybrid retriever. This ensemble approach ensures more robust and comprehensive retrieval results by leveraging both semantic and lexical search techniques.\n",
    "\n",
    "You should:\n",
    "\n",
    "1. Retrieve documents from both Chroma (semantic search) and BM25 (lexical search).\n",
    "2. Combine the results from both retrievers while deduplicating overlapping results.\n",
    "3. Return the top k most relevant and unique documents.\n",
    "\n",
    "This function plays a vital role in the RAG pipeline by ensuring that the retrieved documents are relevant and diverse, combining semantic understanding with precise keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GYdmjXWFtxca"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "class EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    Merges results from Chroma similarity search and BM25 lexical search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chroma_store, bm25_retriever):\n",
    "        \"\"\"\n",
    "        Initialize the EnsembleRetriever with Chroma and BM25 retrievers.\n",
    "\n",
    "        Parameters:\n",
    "        - chroma_store: The Chroma vector store for semantic retrieval.\n",
    "        - bm25_retriever: The BM25 retriever for lexical retrieval.\n",
    "        \"\"\"\n",
    "        # Step 1: Store the Chroma vector store and BM25 retriever.\n",
    "        # Hint: Assign the inputs `chroma_store` and `bm25_retriever` to instance variables.\n",
    "        self.chroma_store = chroma_store  # Replace with your implementation.\n",
    "        self.bm25_retriever = bm25_retriever  # Replace with your implementation.\n",
    "\n",
    "    def get_relevant_documents(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents by combining results from Chroma and BM25.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input search query.\n",
    "        - k (int): The number of top unique documents to return (default: 5).\n",
    "\n",
    "        Returns:\n",
    "        - list[Document]: A list of unique relevant documents.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Retrieve top-k documents from Chroma (semantic similarity).\n",
    "        chroma_docs =  self.chroma_store.similarity_search(query, k=k) # Replace with your implementation.\n",
    "\n",
    "        # Step 2: Retrieve top-k documents from BM25 (lexical matching).\n",
    "        bm25_docs =  self.bm25_retriever.retrieve(query, k=k) # Replace with your implementation.\n",
    "\n",
    "        # Step 3: Combine results from both retrievers into a single list.\n",
    "        combined = chroma_docs + bm25_docs  # Replace with your implementation.\n",
    "\n",
    "        # Step 4: Deduplicate the combined results.\n",
    "        # Hint: Use a `set` to track seen content based on document text.\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc in combined:\n",
    "            # Retrieve content for deduplication (check if `page_content` exists).\n",
    "            # Hint: Use `doc.page_content` if it's a Document object; otherwise, use `doc` as is.\n",
    "            if isinstance(doc, Document):\n",
    "                content = doc.page_content  # Replace with your implementation.\n",
    "            elif isinstance(doc, str):\n",
    "                content = doc\n",
    "            else:\n",
    "                raise ValueError(\"Nothing is expected.\")\n",
    "            # Use the first 60 characters of the document text as a key for deduplication.\n",
    "            key = content[:60]  # Replace with your implementation.\n",
    "\n",
    "            if key not in seen:\n",
    "                # Convert plain strings to Document objects if necessary.\n",
    "                # Hint: Use `Document(page_content=doc)` for plain text.\n",
    "                if isinstance(doc, str):\n",
    "                    doc = Document(page_content=doc)  # Replace with your implementation.\n",
    "                unique_docs.append(doc)\n",
    "                seen.add(key)\n",
    "\n",
    "        # Step 5: Return the top-k unique documents.\n",
    "        return unique_docs[:k]  # Replace with your implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86StDSBJwzYj"
   },
   "source": [
    "Run the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Yz8G5gThtxZf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Retrieval Results:\n",
      "1. Machine learning automates model building using data.\n",
      "2. Deep learning is a type of machine learning using neural networks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning automates model building using data.\"),\n",
    "    Document(page_content=\"Deep learning is a type of machine learning using neural networks.\"),\n",
    "    Document(page_content=\"AI includes technologies like machine learning and deep learning.\"),\n",
    "    Document(page_content=\"Natural language processing focuses on human-computer language interaction.\"),\n",
    "]\n",
    "\n",
    "# Sample Chroma and BM25 retrievers (mock behavior)\n",
    "class MockChroma:\n",
    "    def similarity_search(self, query, k):\n",
    "        return [Document(page_content=\"Machine learning automates model building using data.\")]\n",
    "\n",
    "class MockBM25:\n",
    "    def retrieve(self, query, k):\n",
    "        return [\"Deep learning is a type of machine learning using neural networks.\"]\n",
    "\n",
    "# Initialize mock retrievers\n",
    "chroma = MockChroma()\n",
    "bm25 = MockBM25()\n",
    "\n",
    "# Initialize EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(chroma, bm25)\n",
    "\n",
    "# Test the retriever with a query\n",
    "query = \"What is machine learning?\"\n",
    "results = ensemble_retriever.get_relevant_documents(query, k=5)\n",
    "\n",
    "# Print the results\n",
    "print(\"Ensemble Retrieval Results:\")\n",
    "for idx, doc in enumerate(results, 1):\n",
    "    print(f\"{idx}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5atP_1k5wmr9"
   },
   "source": [
    "Ensemble Retrieval Results:\n",
    "1. Machine learning automates model building using data.\n",
    "2. Deep learning is a type of machine learning using neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp3i6YimxSEl"
   },
   "source": [
    "1. Why is it beneficial to combine semantic retrieval (Chroma) and lexical retrieval (BM25) in an Ensemble Retriever?\n",
    "\n",
    "Answer: We combine semantic retrieval (Chroma) and lexical retrieval (BM25) in an Ensemble Retriever to increase the chances of retrieving the relevant documents because BM25 matches with the exact words and Chroma captures the context and sysnonyms of the words. by combining these both we could get diverse results resulting in higher chances of getting the desired output. Another benefit is Robustness meaning, if the semantic retrieval fails to retrieve the relevant result then BM25 perform well in the same query. Both Chroma and BM25 have limitations, combining them can reduce the weaknesses and creates a balance. BM25 works best for keyword-based queries and Chroma works well for complex queries, combining them can ensures good retrieval results across query types.\n",
    "\n",
    "2. If the results from Chroma and BM25 are drastically different (little to no overlap), how might this impact the quality of the combined results?\n",
    "\n",
    "Answer: it can have a positve or negative impact. \n",
    "\n",
    "Combining both gets diverse range of resulting and more coverage as BM25 and Chroma alone do. when use together, they can compensate for each other's weaknesses. for example, if i ask, \"what is quantum mechanics?\", and in the documents, \"quantum mechanics\" is abbreviated to \"QM\", BM25 can do very poorly. However, Chroma, uses semantic search using embeddings, can help recognizing semantic similarity in the documents, even if the exact keyword doesn't appear. This is particularly useful with synonyms or paraphrases. however, both results may return irrelevance documents, this might the case for producing nonsense sentences in the results texts.\n",
    "It can also bring irrelevant information as one method can retrieves the unrelated information which dilutes the overall quality of the result. There could be a ranking confusion when comining these methods as there are no special ordering, agreement definded between them. So, a high relevant semanctic match could rank lower than less relevant lexical match. \n",
    "It can also lack coherence, users might not be able to find the useful information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Lqc0U7qgtwKs"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "class StrOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpqUJMc2yzS-"
   },
   "source": [
    "# 10. Format Documents\n",
    "\n",
    "In this task, you will implement two key components to enhance the formatting and styling of documents in the RAG pipeline:\n",
    "\n",
    "format_docs(docs):\n",
    "\n",
    "This function takes a list of documents (docs) and formats them into a readable, numbered list. If no documents are provided, it returns a default message indicating the absence of context.\n",
    "\n",
    "style_prompt:\n",
    "\n",
    "This is a prompt template that prepares the input for a neural style transfer task. It asks an AI model to rewrite a given text (original_text) in a specified style, optionally using a contextual snippet (context) from the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3r_YDxMozMZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format a list of documents into a numbered, readable string.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list[Document]): A list of Document objects to format.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string containing the formatted documents or a default message if no documents are provided.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Check if the list of documents is empty.\n",
    "    # Hint: If `docs` is empty, return the string \"No relevant context found.\"\n",
    "    if not docs:\n",
    "        return \"No relevant context found.\"  # Replace with your implementation.\n",
    "\n",
    "    # Step 2: Initialize an empty list to store formatted snippets.\n",
    "    snippet_list = []\n",
    "\n",
    "    # Step 3: Iterate over the documents and format each one.\n",
    "    # - Use `enumerate` to get the index and document.\n",
    "    # - Extract and clean the `page_content` of the document.\n",
    "    # - Replace newlines with spaces and remove unnecessary whitespace.\n",
    "    # - Add a formatted string to the `snippet_list` (e.g., \"1. Cleaned content\").\n",
    "    for i, doc in enumerate(docs):\n",
    "        text = doc.page_content\n",
    "        formatted_string = text.replace('\\n', ' ').strip()\n",
    "        snippet_list.append(f\"{i + 1}. {formatted_string}\")\n",
    "\n",
    "    # Step 4: Join the snippets with newline characters and return the result.\n",
    "    snippet = \"\\n\".join(snippet_list)\n",
    "    return snippet  # Replace with your implementation.\n",
    "\n",
    "\n",
    "# Define the style transfer prompt template\n",
    "style_prompt = PromptTemplate(\n",
    "    input_variables=[\"style\", \"context\", \"original_text\"],\n",
    "    template=(\n",
    "            # Replace with your prompt for changing the style of the text. Avoid using complicated prompts.\n",
    "            \"rewrite the original text: {original_text} with {style} style using the context: {context}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ58VijEzuPH"
   },
   "source": [
    "Execute the following code to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ld8RnXL9zmnw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Documents:\n",
      "\n",
      "1. Machine learning automates data analysis.\n",
      "2. Deep learning uses neural networks to learn patterns.\n",
      "3. Artificial intelligence includes various technologies.\n",
      "\n",
      "Generated Prompt for Style Transfer:\n",
      "\n",
      "rewrite the original text: Artificial intelligence is transforming the world. with poetic style using the context: 1. Machine learning automates data analysis.\n",
      "2. Deep learning uses neural networks to learn patterns.\n",
      "3. Artificial intelligence includes various technologies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/8xy4bq9x7lld84_pt1wqbm4r0000gn/T/ipykernel_18090/3067070408.py:39: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  styled_output = llm(styled_prompt)  # Generate the styled text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Rewritten (Styled) Text ---\n",
      "\n",
      "\n",
      "In a realm of wonder and innovation, artificial intelligence takes flight,\n",
      "A symphony of algorithms and patterns, working with unmatched insight.\n",
      "Machine learning, our humble servant, sifting through data with ease,\n",
      "Analyzing mountains of information, where others are beseeched.\n",
      "\n",
      "Deep learning, the intellect's own child, harnesses the power of neural nets,\n",
      "Learning from the patterns and complexities that often leave us in debt.\n",
      "Neural networks, interwoven strands, work tirelessly to seek,\n",
      "The unspoken patterns within chaos, providing a rich, intricate speak.\n",
      "\n",
      "Artificial intelligence, an expansive tapestry, weaves technologies as one,\n",
      "A merging of minds and machinery, a tale yet undone.\n",
      "With every stitch, every connection, it redefines what we know,\n",
      "Paving the path to a future where progress will continue to glow.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEndpoint  # Or the specific LLM library you're using\n",
    "\n",
    "# Example setup for LLM (ensure this is compatible with your LLM)\n",
    "def setup_llm():\n",
    "    return HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",  # Replace with the appropriate model\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"Machine learning automates data analysis.\"),\n",
    "    Document(page_content=\"Deep learning uses neural networks to learn patterns.\"),\n",
    "    Document(page_content=\"Artificial intelligence includes various technologies.\"),\n",
    "]\n",
    "\n",
    "# Test the format_docs function\n",
    "formatted_docs = format_docs(sample_docs)\n",
    "print(\"Formatted Documents:\\n\")\n",
    "print(formatted_docs)\n",
    "\n",
    "# Test the style_prompt with sample inputs\n",
    "style = \"poetic\"\n",
    "context = formatted_docs\n",
    "original_text = \"Artificial intelligence is transforming the world.\"\n",
    "\n",
    "styled_prompt = style_prompt.format(\n",
    "    style=style,\n",
    "    context=context,\n",
    "    original_text=original_text,\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Prompt for Style Transfer:\\n\")\n",
    "print(styled_prompt)\n",
    "\n",
    "# Pass the prompt to the LLM\n",
    "llm = setup_llm()  # Initialize the LLM\n",
    "styled_output = llm(styled_prompt)  # Generate the styled text\n",
    "\n",
    "print(\"\\n--- Rewritten (Styled) Text ---\")\n",
    "print(styled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq3HaHd0CFog"
   },
   "source": [
    "# 11. RAG chain\n",
    "\n",
    "In this task, students will implement a RAG chain that integrates an ensemble retriever (Chroma and BM25), formats retrieved context, applies a prompt template, and generates styled output using a Language Model (LLM).\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "Use the EnsembleRetriever to retrieve relevant documents from Chroma and BM25.\n",
    "Format the retrieved documents into a readable context.\n",
    "Generate a prompt for neural style transfer using the retrieved context and the input query.\n",
    "Pass the prompt to the LLM and parse the model's response to return the final styled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nbvH2EZ9zp-d"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def build_rag_chain(llm, chroma_store, bm25_retriever):\n",
    "    \"\"\"\n",
    "    Build a RAG chain using an ensemble retriever with Chroma and BM25,\n",
    "    followed by formatting the context, applying the prompt, and parsing the output.\n",
    "\n",
    "    Parameters:\n",
    "    - llm: The language model for generating styled text.\n",
    "    - chroma_store: Chroma vector store for semantic retrieval.\n",
    "    - bm25_retriever: BM25 retriever for lexical retrieval.\n",
    "\n",
    "    Returns:\n",
    "    - rag_chain: A function that processes inputs through the RAG pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Define the Ensemble Retriever\n",
    "    ensemble_retriever = EnsembleRetriever(chroma_store,bm25_retriever)  # Replace with your implementation.\n",
    "\n",
    "    # Step 2: Define a function to retrieve and format context\n",
    "    def retrieve_and_format_context(query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents and format them into a readable context.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): The input query.\n",
    "        - k (int): The number of documents to retrieve (default: 5).\n",
    "\n",
    "        Returns:\n",
    "        - str: The formatted context string.\n",
    "        \"\"\"\n",
    "        # Step 2.1: Retrieve relevant documents using the ensemble retriever.\n",
    "        context_docs = ensemble_retriever.get_relevant_documents(query,k=k)  # Replace with your implementation.\n",
    "\n",
    "        # Step 2.2: Format the retrieved documents.\n",
    "        context = format_docs(context_docs)  # Replace with your implementation.\n",
    "\n",
    "        return context\n",
    "\n",
    "    # Step 3: Define the RAG chain\n",
    "    def rag_chain(inputs):\n",
    "        \"\"\"\n",
    "        Process inputs through the RAG pipeline to generate styled output.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs (dict): A dictionary containing:\n",
    "            - \"question\" (str): The query for retrieving context.\n",
    "            - \"style\" (str): The desired writing style.\n",
    "            - \"original_text\" (str): The text to be rewritten.\n",
    "\n",
    "        Returns:\n",
    "        - str: The final styled output.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 3.1: Retrieve and format the context using the helper function.\n",
    "        query = inputs[\"question\"]\n",
    "        context = retrieve_and_format_context(query,k=5)  # Replace with your implementation.\n",
    "\n",
    "        # Step 3.2: Generate the prompt using the `style_prompt`.\n",
    "        prompt = style_prompt.format(\n",
    "            style=inputs[\"style\"],\n",
    "            context=context,\n",
    "            original_text=inputs[\"original_text\"]\n",
    "        )  # Replace with your implementation.\n",
    "\n",
    "        # Step 3.3: Pass the prompt through the LLM to generate the output.\n",
    "        llm_output = llm(prompt)  # Replace with your implementation.\n",
    "\n",
    "        # Step 3.4: Parse the LLM's output to extract the final styled text.\n",
    "        parse_passthrough = RunnablePassthrough()\n",
    "        parser = parse_passthrough.invoke(llm_output)  # Replace with your implementation.\n",
    "        result = parser  # Replace with your implementation.\n",
    "\n",
    "        return result\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0NBNIFCDgvE"
   },
   "source": [
    "# 12. Final response\n",
    "\n",
    "In this task, students will implement the main script that integrates all components of the RAG pipeline into a complete application. The script will:\n",
    "\n",
    "1. Scrape content from specified URLs, process the raw text, and split it into smaller, retrievable chunks.\n",
    "2. Build the retrievers: Create a Chroma vector store and a BM25 retriever using the processed documents.\n",
    "3. Build the RAG chain: Set up a pipeline that integrates the retrievers, context formatting, and an LLM to perform neural style transfer.\n",
    "4. Run the application: Accept a user query and a target style, then process the input through the RAG chain to produce styled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "I1pFwYzOCZvj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Scraping content and splitting into documents...\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "Scraping content from: https://en.wikipedia.org/wiki/Machine_learning\n",
      "Total number of documents: 820\n",
      "Step 2: Building Chroma vector store and BM25 retriever...\n",
      "Step 3: Building RAG chain...\n",
      "\n",
      "Step 4: Neural Style Transfer Demo...\n",
      "\n",
      "============================================\n",
      "        Neural Style Transfer Demo          \n",
      "============================================\n",
      "Original Text : Explain machine learning.\n",
      "Desired Style : as if it were a recipe for cooking\n",
      "\n",
      "Step 5: Running the RAG chain...\n",
      "\n",
      "--- Styled Output ---\n",
      " usage of machine learning in data compression, in particular adaptive Lempel-Ziv-Welch coding, has been demonstrated to greatly improve upon the performance of simple algorithms like the Huffman code.[35]\n",
      "\n",
      "Preparing the Ingredients for Machine Learning:\n",
      "\n",
      "Gather your data and separate it into labeled and unlabeled sets. For supervised learning, ensure the labeled set is abundant and accurately annotated.\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* Datasets (labeled and unlabeled)\n",
      "* Statistical algorithms\n",
      "* Pattern detection techniques\n",
      "* Neural networks (optional)\n",
      "* Adaptive Lempel-Ziv-Welch coding (optional for data compression)\n",
      "\n",
      "Instructions for Machine Learning:\n",
      "\n",
      "1. Prepare the labeled and unlabeled datasets by organizing the data in a format suitable for statistical analysis.\n",
      "\n",
      "2. Choose a statistical algorithm or pattern detection technique to use for your analysis, depending on the nature of your data and the task at hand.\n",
      "\n",
      "3. Train the algorithm on the labeled dataset by providing it with a set of examples and their corresponding correct answers.\n",
      "\n",
      "4. Allow the algorithm to iteratively adjust its parameters based on its performance on the training data, seeking to minimize the error or maximize the accuracy of its predictions.\n",
      "\n",
      "5. Once the algorithm has been properly trained, test its performance on a separate, unlabeled dataset to evaluate its generalizability.\n",
      "\n",
      "6. Optimize the algorithm by fine-tuning its parameters, or by using additional techniques such as cross-validation, regularization, or ensemble learning.\n",
      "\n",
      "7. Apply the trained algorithm to new, unseen data to make predictions or find patterns, or use it to improve the performance of other AI programs.\n",
      "\n",
      "Optional steps:\n",
      "\n",
      "1. If your dataset is large or complex, consider using a neural network to capture its underlying structure more effectively.\n",
      "2. If you are working with data compression, apply adaptive Lempel-Ziv-Welch coding to improve the performance of your compression algorithm.\n",
      "\n",
      "Sources:\n",
      "[1] Wikipedia. (n.d.). Machine learning. Retrieved March 17, 2023, from <https://en.wikipedia.org/wiki/Machine_learning>\n",
      "[2] Deep learning. (n.d.). In Wikipedia. Retrieved March 17\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main script for scraping, building retrievers, setting up the RAG chain,\n",
    "    and running a neural style transfer demo.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Scrape content and split into documents\n",
    "    print(\"Step 1: Scraping content and splitting into documents...\")\n",
    "    example_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    ]\n",
    "\n",
    "    # Step 1A: Initialize an empty list to store all documents\n",
    "    all_docs = []\n",
    "\n",
    "    # Step 1B: Iterate through the URLs to fetch and process content\n",
    "    for url in example_urls:\n",
    "        print(f\"Scraping content from: {url}\")\n",
    "\n",
    "        # Step 1B.1: Fetch and parse the raw text from the URL\n",
    "        raw_text = fetch_and_parse(url)\n",
    "\n",
    "        # Step 1B.2: Split the raw text into chunks (documents)\n",
    "        splits = split_text_into_documents(raw_text)\n",
    "\n",
    "        # Step 1B.3: Add the chunks to the list of documents\n",
    "        all_docs.extend(splits)\n",
    "\n",
    "    print(f\"Total number of documents: {len(all_docs)}\")\n",
    "\n",
    "    # Step 2: Build Chroma and BM25 retrievers\n",
    "    print(\"Step 2: Building Chroma vector store and BM25 retriever...\")\n",
    "\n",
    "    # Step 2A: Build the Chroma vector store\n",
    "    chroma_store = build_chroma(all_docs)  # Replace with your implementation\n",
    "\n",
    "    # Step 2B: Build the BM25 retriever\n",
    "    bm25_retriever = BM25Retriever(all_docs)  # Replace with your implementation\n",
    "\n",
    "    # Step 3: Build the RAG chain\n",
    "    print(\"Step 3: Building RAG chain...\")\n",
    "\n",
    "    # Step 3A: Set up the LLM\n",
    "    llm = setup_llm()  # Replace with your implementation\n",
    "\n",
    "    # Step 3B: Build the RAG chain\n",
    "    rag_chain = build_rag_chain(llm,chroma_store,bm25_retriever)  # Replace with your implementation\n",
    "\n",
    "    # Step 4: Neural Style Transfer Demo\n",
    "    print(\"\\nStep 4: Neural Style Transfer Demo...\")\n",
    "\n",
    "    # Step 4A: Define the user query and target style\n",
    "    user_text = \"Explain machine learning.\"\n",
    "    target_style = \"as if it were a recipe for cooking\"\n",
    "    inputs = {\"question\": user_text, \"style\": target_style, \"original_text\": user_text}\n",
    "\n",
    "    print(\"\\n============================================\")\n",
    "    print(\"        Neural Style Transfer Demo          \")\n",
    "    print(\"============================================\")\n",
    "    print(f\"Original Text : {user_text}\")\n",
    "    print(f\"Desired Style : {target_style}\")\n",
    "\n",
    "    # Step 5: Run the RAG chain\n",
    "    print(\"\\nStep 5: Running the RAG chain...\")\n",
    "\n",
    "    # Hint: Pass `inputs` through the RAG chain to generate styled output.\n",
    "    styled_result = rag_chain(inputs)  # Replace with your implementation\n",
    "\n",
    "    print(\"\\n--- Styled Output ---\")\n",
    "    print(styled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVYsup1kFTUW"
   },
   "source": [
    "***What You Should Remember:***\n",
    "\n",
    "1. RAG (Retrieval-Augmented Generation) combines the power of information\n",
    "retrieval and language models to generate accurate and context-aware responses.\n",
    "\n",
    "2. Chroma Vector Store is used for semantic retrieval by embedding documents into high-dimensional vectors and finding semantically similar documents for a given query.\n",
    "\n",
    "3. BM25 Retriever uses lexical matching to rank documents based on the occurrence of query terms, ensuring precision in keyword-based searches.\n",
    "\n",
    "4. Ensemble Retriever merges results from Chroma (semantic similarity) and BM25 (lexical matching) to provide a balance of relevance and diversity in retrieved documents.\n",
    "\n",
    "5. Formatting Context ensures that retrieved documents are clean, readable, and useful for the LLM, improving the quality of generated outputs.\n",
    "\n",
    "6. Prompt Templates guide the LLM by structuring inputs, specifying the task (e.g., style transfer), and ensuring clarity and relevance.\n",
    "\n",
    "7. Neural Style Transfer enables the LLM to rewrite text in a specified style (e.g., formal, poetic, conversational) using both the original input and retrieved context.\n",
    "\n",
    "8. Building a RAG pipeline requires:\n",
    "\n",
    "    **Data preparation:** Scraping and splitting raw text into smaller, retrievable chunks.\n",
    "\n",
    "    **Retriever setup:** Combining Chroma and BM25 to maximize retrieval quality.\n",
    "\n",
    "    **Chain integration:** Connecting the retrievers, context formatting, and LLM to form a cohesive workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ1Eo7i5GSDT"
   },
   "source": [
    "Congratulations! You've come to the end of this assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
